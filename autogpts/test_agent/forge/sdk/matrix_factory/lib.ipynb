{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T21:17:11.396642Z",
     "start_time": "2023-10-12T21:17:10.828558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response for \"I am doing well...\" using gpt-3.5-turbo-instruct-0914 in 0.34 seconds.\n",
      "Je vais bien\n",
      "Generated response for \"I love programming...\" using gpt-3.5-turbo-instruct-0914 in 0.34 seconds.\n",
      "J'aime programmer\n",
      "Generated response for \"Thank you very much...\" using gpt-3.5-turbo-instruct in 0.34 seconds.\n",
      "Merci beaucoup\n",
      "Generated response for \"I am doing poorly...\" using gpt-3.5-turbo-instruct in 0.34 seconds.\n",
      "Je ne vais pas bien\n",
      "Generated response for \"Hello, how are you?...\" using gpt-3.5-turbo-instruct in 0.34 seconds.\n",
      "Bonjour, comment ça va ?\n",
      "Responses: in 0.34 seconds. ['Je vais bien', \"J'aime programmer\", 'Merci beaucoup', 'Je ne vais pas bien', 'Bonjour, comment ça va ?']\n"
     ]
    }
   ],
   "source": [
    "from fgn.completion.complete import acreate\n",
    "from matrix_factory.chat_helpers import ok_models, instruct_models\n",
    "import time\n",
    "import openai\n",
    "import anyio\n",
    "from typing import Iterable, List\n",
    "import itertools\n",
    "\n",
    "\n",
    "# 1. Model Selection\n",
    "def model_generator(models: List[str]):\n",
    "    \"\"\"\n",
    "    A generator that yields models in a round-robin fashion using itertools.cycle.\n",
    "    \"\"\"\n",
    "    return itertools.cycle(models)\n",
    "\n",
    "# 2. Prompt Generation (could be expanded if there are more prompt types)\n",
    "def construct_openai_prompt(base_prompt: str, item: str) -> str:\n",
    "    return f\"{base_prompt}: {item}\"\n",
    "\n",
    "# 3. OpenAI Call\n",
    "async def call_openai(prompt: str, model_name: str, max_tokens: int = 50) -> str:\n",
    "    return await acreate(prompt=prompt, model=model_name, max_tokens=max_tokens)\n",
    "\n",
    "\n",
    "# 4. Timing and Logging\n",
    "async def timed_openai_call(prompt: str, item: str, model_name: str, max_tokens: int = 50) -> str:\n",
    "    start_time = time.time()\n",
    "    response = await call_openai(prompt, model_name, max_tokens)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Generated response for \\\"{item[:30]}...\\\" using {model_name} in {elapsed_time:.2f} seconds.\")\n",
    "    return response\n",
    "\n",
    "# 5. Concurrent Execution\n",
    "async def prompt_map(base_prompt: str, iterable: Iterable[str], max_tokens: int = 50, model_list: List[str] = None) -> List[str]:\n",
    "    model_gen = model_generator(model_list or instruct_models)\n",
    "    responses = []\n",
    "\n",
    "    async def generate_response(item: str) -> None:\n",
    "        model_name = next(model_gen)\n",
    "        prompt = construct_openai_prompt(base_prompt, item)\n",
    "        response = await timed_openai_call(prompt, item, model_name, max_tokens)\n",
    "        print(response)\n",
    "        responses.append(response)\n",
    "\n",
    "    async with anyio.create_task_group() as tg:\n",
    "        for item in iterable:\n",
    "            tg.start_soon(generate_response, item)\n",
    "\n",
    "    return responses\n",
    "\n",
    "# [Rest of your functions and example usage]\n",
    "\n",
    "# Example models and usage:\n",
    "\n",
    "example_prompt = \"Translate the following text into French\"\n",
    "text_list = [\"Hello, how are you?\", \"I love programming\", \"Thank you very much\", \"I am doing well\", \"I am doing poorly\"]\n",
    "start_time = time.time()\n",
    "responses_best_models = await prompt_map(example_prompt, text_list, 50)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Responses: in {elapsed_time:.2f} seconds.\", responses_best_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response for \"The sun is shining today....\" using gpt-3.5-turbo-instruct in 0.24 seconds.\n",
      "true\n",
      "Generated response for \"The flowers are blooming....\" using gpt-3.5-turbo-instruct in 0.25 seconds.\n",
      "true\n",
      "Generated response for \"I aced the exam!...\" using gpt-3.5-turbo-instruct in 0.25 seconds.\n",
      "true\n",
      "Generated response for \"The weather is gloomy....\" using gpt-3.5-turbo-instruct-0914 in 0.25 seconds.\n",
      "true\n",
      "Generated response for \"I failed the test....\" using gpt-3.5-turbo-instruct-0914 in 0.27 seconds.\n",
      "true\n",
      "Positive statements: ['The sun is shining today.', 'I failed the test.', 'The flowers are blooming.', 'The weather is gloomy.', 'I aced the exam!']\n"
     ]
    }
   ],
   "source": [
    "async def prompt_filter(base_prompt: str, iterable: Iterable[str], model_list: List[str] = None, max_tokens: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    This function takes a base prompt and filters an iterable based on responses from OpenAI.\n",
    "\n",
    "    Args:\n",
    "        base_prompt (str): The base prompt for generating boolean responses.\n",
    "        iterable (iterable): An iterable (e.g., list, tuple) of strings to be filtered.\n",
    "        model_list (List[str]): List of models to be used in round-robin fashion. If None, defaults to instruct_models.\n",
    "        max_tokens (int): The maximum number of tokens in each response.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of items from the iterable that pass the condition specified by the prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    responses = await prompt_map(base_prompt, iterable, max_tokens, model_list)\n",
    "    results = [item for item, resp in zip(iterable, responses) if resp.strip().lower() == \"true\"]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "filter_prompt = \"Based on the following text, provide a boolean response ('true' or 'false'): \"\n",
    "\n",
    "\n",
    "statements = [\n",
    "    \"The sun is shining today.\",\n",
    "    \"I failed the test.\",\n",
    "    \"The flowers are blooming.\",\n",
    "    \"The weather is gloomy.\",\n",
    "    \"I aced the exam!\"\n",
    "]\n",
    "\n",
    "filtered_statements = await prompt_filter(filter_prompt, statements)\n",
    "print(\"Positive statements:\", filtered_statements)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T21:19:33.440945Z",
     "start_time": "2023-10-12T21:19:33.163459Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T21:14:21.597594Z",
     "start_time": "2023-10-12T21:14:21.593952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
